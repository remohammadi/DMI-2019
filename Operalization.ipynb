{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1122936 tweets from 770 users\n"
     ]
    }
   ],
   "source": [
    "from os.path import join\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "with open(join(\"data\",\"iranian_users.csv\")) as csvfile:\n",
    "    csvreader = csv.reader(csvfile, delimiter=',')\n",
    "    users = {row[2]: {'row': row, 'tweets': [], 'scores': None} for row in csvreader if row[0] != 'userid'}\n",
    "\n",
    "tweet_counter = 0\n",
    "missing_users = set([])\n",
    "with open(join(\"data\",\"iranian_tweets.csv\")) as csvfile:\n",
    "    csvreader = csv.reader(csvfile, delimiter=',')\n",
    "    for tweet in csvreader:\n",
    "        screen_name = tweet[3]\n",
    "        if screen_name not in users:\n",
    "            missing_users.add(screen_name)\n",
    "        else:\n",
    "            users[screen_name]['tweets'].append(tweet)\n",
    "            tweet_counter += 1\n",
    "\n",
    "missing_users.remove('user_screen_name')\n",
    "if missing_users:\n",
    "    print(\"Missing users with tweets: \" + ', '.join(list(not_in_sources)))\n",
    "    print()\n",
    "\n",
    "print(\"Loaded {tn} tweets from {un} users\".format(tn=tweet_counter, un=len(users)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "\n",
    "DATASET_DATE = datetime(2018, 9, 1)\n",
    "\n",
    "# Load the Media Bias/Fact Check db\n",
    "with open(join('data', 'sources.json')) as infile:\n",
    "    sources = json.load(infile)\n",
    "\n",
    "FACTUAL_HALF = set(['MIXED'])\n",
    "FACTUAL_COMPLETELY = set(['HIGH', 'VERY HIGH'])\n",
    "\n",
    "# A failed attemp to write a function to decode the reported_location field\n",
    "# import pycountry\n",
    "# _countries_cache = {}\n",
    "# def get_country(name):\n",
    "#     c = _countries_cache.get(name, None)\n",
    "#     if name not in _countries_cache:\n",
    "#         c = pycountry.countries.get(name=name)\n",
    "#         if not c and len(name) == 2:\n",
    "#             c = pycountry.countries.get(alpha_2=name.upper())\n",
    "#         if not c and len(name) == 3:\n",
    "#             c = pycountry.countries.get(alpha_3=name.upper())\n",
    "#         if not c:\n",
    "#             c = pycountry.countries.get(official_name=name)\n",
    "#         _countries_cache[name] = c\n",
    "#         if not country:\n",
    "#             print(\"failed to find country for \" + name)\n",
    "#     return c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the Scores for each User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57e2082d64baa89d:   1.76 = (0.3, 1.0, 0.5, 0.0)\n",
      "11891c406c088fdd:   2.08 = (1.0, 0.1, 1.0, 0.0)\n",
      "5ddbd530097789a4:   1.10 = (0.3, 0.2, 0.4, 0.2)\n",
      "50012d5e4f959a3d:   1.99 = (0.6, 0.2, 0.5, 0.8)\n",
      "28478f20c217a672:   1.93 = (0.1, 1.0, 0.8, 0.0)\n",
      "260bbf8c9ca24c63:   1.87 = (0.3, 1.0, 0.6, 0.0)\n",
      "3f1a40fa0636db86:   1.55 = (0.1, 1.0, 0.4, 0.0)\n",
      "bc2adb39c920650d:   2.02 = (0.3, 1.0, 0.7, 0.1)\n",
      "fd01dd625797c4f4:   1.77 = (0.2, 1.0, 0.6, 0.0)\n",
      "2d761afd8c25c25e:   1.67 = (0.1, 1.0, 0.5, 0.0)\n",
      "f3776839f137b8fd:   2.26 = (0.6, 1.0, 0.6, 0.0)\n",
      "c2577510f436a286:   1.83 = (0.3, 1.0, 0.6, 0.0)\n",
      "0669e7730d2cacb6:   1.66 = (0.1, 1.0, 0.6, 0.0)\n",
      "\n",
      "7 users are suspended without having any tweets in the dataset: 79b10b6396d4c863, 6e395ce51ae771f4, 1c2bac6ec8313377, caaa14567470b25d, f3d8b36e08acc468, 8b1498b609a3e879, 299d67846470c44d\n",
      "\n",
      "Domains not in the Media Bias/Fact Check db: jamieoliver.com, fb.me, mjhosts.com, shar.es, reddit.com, aynanewsagency.or, sarahabed.com, mirataljazeera.org, ln.is, slate.me, alalam.ir, venturebeat.com, rsph.org.uk, compelling.org.uk, institutomanquehue.org,, owl.li, bet.us, cnn.it, reut.rs, aje.io, actblue.com, ift.tt, crwd.fr, soundcloud.com, hill.cm, bet.com, misspeachy.co, thenewkhalij.news, wxch.nl, activistpost.com, mehrnews.com, sptnkne.ws, youtube.com, 21stcenturywire.com, nyti.ms, trib.al, aynanewsagency.org, chuffed.org, nydn.us, importedfun.co, altervista.org, ddnnews.com, payement.pw, newsbtc.com, farsnews.com, iuvmonline.com, rand.org,, b2blistings.org, abacusnews.com, bit.ly, power-technology.com, almanar.com.lb, reddit.co, howsecureismypassword.net, facebook.com, daysofpalestine.com, amn.st, j.mp, callapp.com, atfp.co, 9gag.com, adbl.co, thr.cm, granma.cu, palinfo.com, cbs5az.com, youtu.be, betbitcoin.pr, pscp.tv, goo.gl, ahtribune.com, apne.ws, huff.to, presstv.ir, institutomanquehue.org, awdnews.com, noticias.yahoo.com, fxn.ws, aparat.com, instagram.com, upflow.co, statista.com, khamenei.ir, malayalamsearch.com, twitter.com, ind.pn, libertyfrontpress.com, nbcnews.to, wapo.st, dlvr.it, bild.de, ti.me, shopnet.one, ptv.io, mashreghnews.ir, ow.ly, anonsworldwide.com, wordpress.com, iuvmpress.com, norsecorp.co, nolalikes.com, bit.do, theregister.co.uk, nejatngo.org, buff.ly, dci-palestine.or, trendcityradio.com, fr-online.de, t.me, abna.cc, aml.ink, maannews.com, herb.co, bzfd.it, tamasha.com, nwk.ee, liberalresistance.net\n",
      "\n",
      "Overall factuality of the links in the tweets:{'HIGH': 38, 'MIXED': 22, '': 35, 'VERY HIGH': 1}\n"
     ]
    }
   ],
   "source": [
    "not_in_sources = set([])\n",
    "dataset_sources_factuality = {}\n",
    "\n",
    "\n",
    "users_with_no_tweet = set([])\n",
    "max_account_age = None\n",
    "min_account_age = None\n",
    "\n",
    "user_counter = 0\n",
    "for screen_name, profile in users.items():\n",
    "    n = len(profile['tweets'])\n",
    "\n",
    "    interface_language = profile['row'][9].lower()[:2]\n",
    "    timezone = profile['row'][9].lower()[:2]\n",
    "    account_age = (DATASET_DATE - datetime.strptime(profile['row'][8], '%Y-%m-%d')).days\n",
    "    if not max_account_age or max_account_age < account_age:\n",
    "        max_account_age = account_age\n",
    "    if not min_account_age or min_account_age > account_age:\n",
    "        min_account_age = account_age\n",
    "#     reported_location = profile['row'][3]\n",
    "#     if reported_location:\n",
    "#         reported_location = reported_location.split(',')[-1].strip()\n",
    "#         country = get_country(reported_location)\n",
    "\n",
    "    if n == 0:\n",
    "        users_with_no_tweet.add(screen_name)\n",
    "        profile['scores'] = None\n",
    "        continue\n",
    "\n",
    "    language_score_unmatched = 0\n",
    "    tweet_time_bin = [0] * 24\n",
    "    linked_to_not_factual_domain = 0\n",
    "    linked_to_known_domain = 0\n",
    "    for tweet in profile['tweets']:\n",
    "        if tweet[11][:2].lower() != interface_language:\n",
    "            language_score_unmatched += 1\n",
    "\n",
    "        tweet_time = datetime.strptime(tweet[13], '%Y-%m-%d %H:%M')\n",
    "        tweet_time_bin[tweet_time.hour] += 1\n",
    "\n",
    "        urls = tweet[-3][2:-2].split()\n",
    "        for url in urls:\n",
    "            domain = urlparse(url).netloc.lower()\n",
    "            if domain not in sources and domain.count('.') > 1:\n",
    "                domain = domain[domain.index('.')+1:]\n",
    "            if domain not in sources:\n",
    "                not_in_sources.add(domain)\n",
    "                continue\n",
    "            linked_to_known_domain += 1\n",
    "            factual = sources[domain][0]['factual']\n",
    "            if factual not in dataset_sources_factuality:\n",
    "                dataset_sources_factuality[factual] = 1\n",
    "            else:\n",
    "                dataset_sources_factuality[factual] += 1\n",
    "            if factual in FACTUAL_HALF:\n",
    "                linked_to_not_factual_domain += 0.5\n",
    "            elif factual not in FACTUAL_COMPLETELY:\n",
    "                linked_to_not_factual_domain += 1\n",
    "\n",
    "    max_sliding_time_window_sum = 0\n",
    "    for i in range(24):\n",
    "        sliding_time_window_sum = sum(tweet_time_bin[i:(i+8) % 24])\n",
    "        if sliding_time_window_sum > max_sliding_time_window_sum:\n",
    "            max_sliding_time_window_sum = sliding_time_window_sum\n",
    "\n",
    "    # Normalization\n",
    "    language_score = language_score_unmatched / n\n",
    "    working_time_score = ((max_sliding_time_window_sum / n) - (1/3)) * 3 / 2\n",
    "    if linked_to_known_domain > 0:\n",
    "        linked_to_domains_score = linked_to_not_factual_domain / linked_to_known_domain\n",
    "    else:\n",
    "        linked_to_domains_score = 0\n",
    "\n",
    "    profile['scores'] = [\n",
    "        account_age,  # To be normalized\n",
    "        language_score,\n",
    "        working_time_score,\n",
    "        linked_to_domains_score\n",
    "    ]\n",
    "\n",
    "    if user_counter == 12:\n",
    "        break\n",
    "    user_counter += 1\n",
    "\n",
    "user_counter = 0\n",
    "for screen_name, profile in users.items():\n",
    "    if profile['scores']:\n",
    "        if max_account_age and min_account_age and max_account_age > min_account_age:\n",
    "            profile['scores'][0] = (profile['scores'][0] - min_account_age) / (max_account_age - min_account_age)\n",
    "\n",
    "        score = sum(profile['scores'])\n",
    "        print('{name}: {score:6.2f} = ({vector})'.format(\n",
    "            name=screen_name[:16], score=score, vector=', '.join(['{0:3.1f}'.format(v) for v in profile['scores']])))\n",
    "        if user_counter == 12:\n",
    "            break\n",
    "        user_counter += 1\n",
    "\n",
    "print()\n",
    "print(\"{num} users are suspended without having any tweets in the dataset: {list}\".format(\n",
    "    num=len(users_with_no_tweet), list=', '.join([s[:16] for s in users_with_no_tweet])))\n",
    "print()\n",
    "print(\"Domains not in the Media Bias/Fact Check db: \" + ', '.join(list(not_in_sources)))\n",
    "print()\n",
    "print(\"Overall factuality of the links in the tweets:\" + str(dataset_sources_factuality))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not found: user_screen_name\n"
     ]
    }
   ],
   "source": [
    "from os.path import join\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "with open(join(\"data\",\"iranian_users.csv\")) as csvfile:\n",
    "    csvreader = csv.reader(csvfile, delimiter=',')\n",
    "    users = {row[2]: {'row': row, 'tweets': [], 'scores': []} for row in list(csvreader)[1:]}\n",
    "\n",
    "with open(join(\"data\",\"iranian_tweets.csv\")) as csvfile:\n",
    "    csvreader = csv.reader(csvfile, delimiter=',')\n",
    "    for tweet in csvreader:\n",
    "        screen_name = tweet[3]\n",
    "        if screen_name not in users:\n",
    "            print(\"Not found: \" + screen_name)\n",
    "        else:\n",
    "            users[screen_name]['tweets'].append(tweet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(join('data', 'sources.json')) as infile:\n",
    "    sources = json.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.9921208141825345, 0.6559422193040052, 0)\n",
      "\n",
      "(0.12903225806451613, 0.967741935483871, 0)\n",
      "\n",
      "(0.21075984470327233, 0.5973377703826955, 0.23684210526315788)\n",
      "\n",
      "(0.1541038525963149, 0.6348408710217756, 0.7708333333333334)\n",
      "\n",
      "(1.0, 0.868421052631579, 0)\n",
      "\n",
      "(1.0, 0.7169230769230769, 0)\n",
      "\n",
      "(1.0, 0.6, 0)\n",
      "\n",
      "(0.9795918367346939, 0.7931972789115647, 0.1)\n",
      "\n",
      "(0.9913606911447084, 0.7192224622030238, 0)\n",
      "\n",
      "(1.0, 0.6934306569343066, 0)\n",
      "\n",
      "(0.9972144846796658, 0.7465181058495822, 0)\n",
      "\n",
      "(1.0, 0.7033898305084746, 0)\n",
      "\n",
      "7 users are suspended without having any tweets in the dataset: 79b10b6396d4c863, 6e395ce51ae771f4, f3d8b36e08acc468, 8b1498b609a3e879, 1c2bac6ec8313377, caaa14567470b25d, 299d67846470c44d\n",
      "\n",
      "Domains not in Media Bias/Fact Check db: howsecureismypassword.net, hill.cm, owl.li, slate.me, amn.st, venturebeat.com, maannews.com, palinfo.com, compelling.org.uk, misspeachy.co, nejatngo.org, malayalamsearch.com, farsnews.com, buff.ly, youtube.com, facebook.com, ln.is, thr.cm, atfp.co, activistpost.com, libertyfrontpress.com, chuffed.org, bzfd.it, 9gag.com, shopnet.one, ow.ly, alalam.ir, bit.do, wapo.st, mashreghnews.ir, ind.pn, institutomanquehue.org, j.mp, aparat.com, aje.io, aml.ink, ptv.io, b2blistings.org, mehrnews.com, iuvmonline.com, youtu.be, crwd.fr, khamenei.ir, upflow.co, iuvmpress.com, thenewkhalij.news, awdnews.com, tamasha.com, pscp.tv, t.me, payement.pw, newsbtc.com, jamieoliver.com, bet.us, reddit.com, bet.com, fb.me, abacusnews.com, goo.gl, institutomanquehue.org,, daysofpalestine.com, nbcnews.to, noticias.yahoo.com, anonsworldwide.com, reut.rs, bild.de, trendcityradio.com, nydn.us, norsecorp.co, cnn.it, 21stcenturywire.com, importedfun.co, rand.org,, nwk.ee, rsph.org.uk, herb.co, huff.to, aynanewsagency.org, soundcloud.com, dlvr.it, nolalikes.com, altervista.org, mjhosts.com, ift.tt, aynanewsagency.or, trib.al, ti.me, ddnnews.com, apne.ws, power-technology.com, actblue.com, theregister.co.uk, granma.cu, twitter.com, sptnkne.ws, fr-online.de, statista.com, nyti.ms, abna.cc, adbl.co, dci-palestine.or, mirataljazeera.org, sarahabed.com, cbs5az.com, betbitcoin.pr, almanar.com.lb, instagram.com, callapp.com, shar.es, bit.ly, wxch.nl, fxn.ws, wordpress.com, reddit.co, presstv.ir, liberalresistance.net, ahtribune.com\n",
      "\n",
      "Overall factuality of the links in the tweets:{'HIGH': 38, 'MIXED': 22, '': 35, 'VERY HIGH': 1}\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import pycountry\n",
    "\n",
    "\n",
    "_countries_cache = {}\n",
    "def get_country(name):\n",
    "    c = _countries_cache.get(name, None)\n",
    "    if name not in _countries_cache:\n",
    "        c = pycountry.countries.get(name=name)\n",
    "        if not c and len(name) == 2:\n",
    "            c = pycountry.countries.get(alpha_2=name.upper())\n",
    "        if not c and len(name) == 3:\n",
    "            c = pycountry.countries.get(alpha_3=name.upper())\n",
    "        if not c:\n",
    "            c = pycountry.countries.get(official_name=name)\n",
    "        _countries_cache[name] = c\n",
    "        if not country:\n",
    "            print(\"failed to find country for \" + name)\n",
    "    return c\n",
    "\n",
    "\n",
    "not_in_sources = set([])\n",
    "dataset_sources_factuality = {}\n",
    "\n",
    "\n",
    "user_counter = 0\n",
    "users_with_no_tweet = set([])\n",
    "for screen_name, profile in users.items():\n",
    "    n = len(profile['tweets'])\n",
    "\n",
    "    interface_language = profile['row'][9].lower()[:2]\n",
    "    timezone = profile['row'][9].lower()[:2]\n",
    "#     reported_location = profile['row'][3]\n",
    "#     if reported_location:\n",
    "#         reported_location = reported_location.split(',')[-1].strip()\n",
    "#         country = get_country(reported_location)\n",
    "\n",
    "    if n == 0:\n",
    "        users_with_no_tweet.add(screen_name)\n",
    "        profile['scores'] = None\n",
    "        continue\n",
    "\n",
    "    language_score_unmatched = 0\n",
    "    tweet_time_bin = [0] * 24\n",
    "    linked_to_not_factual_domain = 0\n",
    "    linked_to_known_domain = 0\n",
    "    for tweet in profile['tweets']:\n",
    "        if tweet[11][:2].lower() != interface_language:\n",
    "            language_score_unmatched += 1\n",
    "\n",
    "        tweet_time = datetime.strptime(tweet[13], '%Y-%m-%d %H:%M')\n",
    "        tweet_time_bin[tweet_time.hour] += 1\n",
    "\n",
    "        urls = tweet[-3][2:-2].split()\n",
    "        for url in urls:\n",
    "            domain = urlparse(url).netloc.lower()\n",
    "            if domain not in sources and domain.count('.') > 1:\n",
    "                domain = domain[domain.index('.')+1:]\n",
    "            if domain not in sources:\n",
    "                not_in_sources.add(domain)\n",
    "                continue\n",
    "            linked_to_known_domain += 1\n",
    "            factual = sources[domain][0]['factual']\n",
    "            if factual not in dataset_sources_factuality:\n",
    "                dataset_sources_factuality[factual] = 1\n",
    "            else:\n",
    "                dataset_sources_factuality[factual] += 1\n",
    "            if factual == 'MIXED':\n",
    "                linked_to_not_factual_domain += 0.5\n",
    "            elif factual != 'HIGH':\n",
    "                linked_to_not_factual_domain += 1\n",
    "\n",
    "    max_sliding_time_window_sum = 0\n",
    "    for i in range(24):\n",
    "        sliding_time_window_sum = sum(tweet_time_bin[i:(i+8) % 24])\n",
    "        if sliding_time_window_sum > max_sliding_time_window_sum:\n",
    "            max_sliding_time_window_sum = sliding_time_window_sum\n",
    "\n",
    "    # Normalization\n",
    "    language_score = language_score_unmatched / n\n",
    "    working_time_score = max_sliding_time_window_sum / n\n",
    "    if linked_to_known_domain > 0:\n",
    "        linked_to_domains_score = linked_to_not_factual_domain / linked_to_known_domain\n",
    "    else:\n",
    "        linked_to_domains_score = 0\n",
    "\n",
    "    profile['scores'] = (language_score, working_time_score, linked_to_domains_score)\n",
    "\n",
    "    print(profile['scores'])\n",
    "    print()\n",
    "    user_counter += 1\n",
    "    if user_counter == 12:\n",
    "        break\n",
    "\n",
    "print(\"{num} users are suspended without having any tweets in the dataset: {list}\".format(\n",
    "    num=len(users_with_no_tweet), list=', '.join([s[:16] for s in users_with_no_tweet])))\n",
    "print()\n",
    "print(\"Domains not in Media Bias/Fact Check db: \" + ', '.join(list(not_in_sources)))\n",
    "print()\n",
    "print(\"Overall factuality of the links in the tweets:\" + str(dataset_sources_factuality))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
